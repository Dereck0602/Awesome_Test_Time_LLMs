<div align="center">
<h1>Awesome Test-time Computing</h1>
</div>

## Test-time Adaptation
### Updating the Model
* **Test-Time Training with Self-Supervision for Generalization under Distribution Shifts** [arxiv 2019.9.29] [pdf](https://arxiv.org/pdf/1909.13231)
  * Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, Moritz Hardt
  * University of California, Berkeley
* **MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption** [arxiv 2022.1.20] [pdf](https://arxiv.org/pdf/2103.16201) [github](https://github.com/AlexanderBartler/MT3)
  * Alexander Bartler, Andre Bühler, Felix Wiewel, Mario Döbler, Bin Yang
  * Institute of Signal Processing and System Theory, University of Stuttgart, Germany
* **Test-Time Training with Masked Autoencoders** [arxiv 2022.9.15] [pdf](https://arxiv.org/pdf/2209.07522)
  * Yossi Gandelsman, Yu Sun, , Xinlei Chen, Alexei A. Efros
  * UC Berkeley, Meta AI
* **TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?** [NeurIPS 2021] [pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/b618c3210e934362ac261db280128c22-Paper.pdf) [github](https://github.com/vita-epfl/ttt-plus-plus)
  * Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, Alexandre Alahi
  * École Polytechnique Fédérale de Lausanne
* **Efficient Test-Time Prompt Tuning for Vision-Language Models** [arxiv 2024.8.11] [pdf](https://arxiv.org/pdf/2408.05775)
  * Yuhan Zhu, Guozhen Zhang, Chen Xu, Haocheng Shen, Xiaoxin Chen, Gangshan Wu, Limin Wang
  * State Key Laboratory for Novel Software Technology, Nanjing University, vivo AI Lab 3Shanghai AI Laboratory
* **Tent: Fully Test-time Adaptation by Entropy Minimization** [arxiv 2021.3.18] [pdf](https://arxiv.org/pdf/2006.10726) [github](https://github.com/vita-epfl/ttt-plus-plus)
  * Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, Trevor Darrell
  * UC Berkeley, Adobe Research
* **MEMO: Test Time Robustness via Adaptation and Augmentation** [arxiv 2022.10.10] [pdf](https://arxiv.org/pdf/2110.09506) [github](https://github.com/zhangmarvin/memo)
  * Marvin Zhang, Sergey Levine, Chelsea Finn
  * UC Berkeley, Stanford University
* **The Entropy Enigma: Success and Failure of Entropy Minimization** [arxiv 2024.5.12] [pdf](https://arxiv.org/pdf/2405.05012) [github](https://github.com/oripress/EntropyEnigma)
  * Ori Press, Ravid Shwartz-Ziv, Yann LeCun, Matthias Bethge
  * University of Tubingen, Tubingen AI Center, New York University, Meta AI, FAIR.
* **On Pitfalls of Test-Time Adaptation** [arxiv 2023.6.6] [pdf](https://arxiv.org/pdf/2306.03536) [github](https://github.com/lins-lab/ttab)
  * Hao Zhao, Yuejiang Liu, Alexandre Alahi, Tao Lin
  * École Polytechnique Fédérale de Lausanne (EPFL), Research Center for Industries of the Future, Westlake University, School of Engineering, Westlake University
* **Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering** [EMNLP 2023] [pdf](https://aclanthology.org/2023.emnlp-main.803.pdf) [github](https://github.com/yisunlp/Anti-CF)
  * Yi Su, Yixin Ji, Juntao Li, Hai Ye, Min Zhang
  * Institute of Computer Science and Technology, Soochow University, China, Department of Computer Science, National University of Singapore
* **Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization** [arxiv 2024.1.11] [pdf](https://arxiv.org/pdf/2311.01459) 
  * Jameel Hassan, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Shahbaz Khan, Salman Khan
  * Mohamed Bin Zayed University of AI, Linköping University, Australian National University
* **Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach** [arxiv 2024.8.14] [pdf](https://arxiv.org/pdf/2408.07511) [github](https://github.com/yarinbar/poem)
  * Yarin Bar, Shalev Shaer, Yaniv Romano
  * Department of Computer Science, Technion IIT, Israel, Department of Electrical and Computer Engineering, Technion IIT, Israel
* **Simulating Bandit Learning from User Feedback for Extractive Question Answering** [ACL 2022] [pdf](https://aclanthology.org/2022.acl-long.355.pdf) [github](https://github.com/lil-lab/bandit-qa)
  * Ge Gao, Eunsol Choi, Yoav Artzi
  * Department of Computer Science and Cornell Tech, Cornell University, Department of Computer Science, The University of Texas at Austin
* **Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment** [ACL 2022] [pdf](https://aclanthology.org/2022.findings-acl.75.pdf) 
  * Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie C.K. Cheung, Siva Reddy
  * Mila, McGill University, University of California, Los Angeles
* **Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.47.pdf) [github](https://github.com/lil-lab/bandit-qa)
  * Runzhe Zhan, Xuebo Liu, Derek F. Wong, Cuilian Zhang, Lidia S. Chao, Min Zhang
  * NLPCT Lab, Department of Computer and Information Science, University of Macau, Institute of Computing, Harbin Institute of Technology, Shenzhen, China
* **COMET: A Neural Framework for MT Evaluation** [EMNLP 2020] [pdf](https://aclanthology.org/2020.emnlp-main.213.pdf) 
  * Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie
  * Unbabel AI
* **Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models** [arxiv 2023.5.29] [pdf](https://arxiv.org/pdf/2305.18010) [github](https://github.com/lil-lab/bandit-qa)
  * Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yi Yang
  * ReLER Lab, AAII, University of Technology Sydney, ReLER Lab, CCAI, Zhejiang University, Stanford University, Baidu Inc.
* **Improving robustness against common corruptions by covariate shift adaptation** [arxiv 2020.10.23] [pdf](https://arxiv.org/pdf/2006.16971) 
  * Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, Matthias Bethge
  * University of Tübingen & IMPRS-IS, LMU Munich, University of Tübingen
* **Selective Annotation Makes Language Models Better Few-Shot Learners** [arxiv 2022.9.5] [pdf](https://arxiv.org/pdf/2209.01975) [github](https://github.com/xlang-ai/icl-selective-annotation)
  * Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu
  * The University of Hong Kong, University of Washington, Allen Institute for AI, Carnegie Mellon University FPenn State University, Meta AI
* **Test-Time Adaptation with Perturbation Consistency Learning** [arxiv 2023.4.25] [pdf](https://arxiv.org/pdf/2304.12764) 
  * Yi Su, Yixin Ji, Juntao Li, Hai Ye, Min Zhang
  * Equal contribution, Institute of Computer Science and Technology, Soochow University, China Department of Computer Science, National University of Singapore, Singapore.
* **Test-Time Prompt Adaptation for Vision-Language Models** [NeurIPS 2023] [pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/cdd0640218a27e9e2c0e52e324e25db0-Paper-Conference.pdf) 
  * Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, Chaowei Xiao
  * University of Maryland, NVIDIA, Caltech, Arizona State University
* **Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning** [arxiv 2023.8.17] [pdf](https://arxiv.org/pdf/2308.06038) [github](https://github.com/chunmeifeng/DiffTPT)
  * Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, Wangmeng Zuo
  * Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), UAE, Australian National University, Canberra ACT, Australia, Harbin Institute of Technology, Harbin, China
* **Test-Time Model Adaptation with Only Forward Passes** [arxiv 2024.5.29] [pdf](https://arxiv.org/pdf/2404.01650) [github](https://github.com/mr-eggplant/FOA)
  * Shuaicheng Niu, Chunyan Miao, Guohao Chen, Pengcheng Wu, Peilin Zhao
  * College of Computing and Data Science, Nanyang Technological University, Singapore, Joint NTU-WeBank Research Centre onFintech, Singapore, Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Singapore, Tencent AI Lab, Shenzhen, China.
* **Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot Generalization of Vision-Language Models** [arxiv 2024.7.22] [pdf](https://arxiv.org/pdf/2407.15913) [github](https://github.com/Razaimam45/TTL-Test-Time-Low-Rank-Adaptation)
  * Raza Imam, Hanan Gani, Muhammad Huzaifa, Karthik Nandakumar
  * Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), UAE
* **StreamAdapter: Efficient Test Time Adaptation from Contextual Streams** [arxiv 2024.11.14] [pdf](https://arxiv.org/pdf/2411.09289) 
  * Dilxat Muhtar, Yelong Shen, Yaming Yang, Xiaodong Liu, Yadong Lu, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Xueliang Zhang, Jianfeng Gao, Weizhu Chen, Qi Zhang
  * Nanjing University, Microsoft
* **Towards Stable Test-time Adaptation in Dynamic Wild World** [arxiv 2023.2.24] [pdf](https://arxiv.org/pdf/2302.12400) [github](https://github.com/mr-eggplant/SAR)
  * Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, Mingkui Tan
  * South China University of Technology, Tencent AI Lab, National University of Singapore, Key Laboratory of Big Data and Intelligent Robot, Ministry of Education4 Pazhou Laboratory
* **SoTTA: Robust Test-Time Adaptation on Noisy Data Streams** [arxiv 2023.10.16] [pdf](https://arxiv.org/pdf/2310.10074) [github](https://github.com/taeckyung/SoTTA)
  * Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chottananurak, Sung-Ju Lee
  * Nokia Bell Labs, KAIST
* **Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time** [arxiv 2022.7.1] [pdf](https://arxiv.org/pdf/2203.05482) [github](https://github.com/mlfoundations/model-soups)
  * Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt
  * University of Washington, Columbia University, Google Research, Brain Team, Meta AI Research, TelAviv University
* **Robust Question Answering against Distribution Shifts with Test-Time Adaption: An Empirical Study** [EMNLP 2022] [pdf](https://aclanthology.org/2022.findings-emnlp.460.pdf) [github](https://github.com/oceanypt/coldqa-tta)
  * Hai Ye, Yuyang Ding, Juntao Li, Hwee Tou Ng
  * Department of Computer Science, National University of Singapore, Soochow University, China
### Modifying the Input
* **What Makes Good In-Context Examples for GPT-3?** [DeeLIO 2022] [pdf](https://aclanthology.org/2022.deelio-1.10.pdf) 
  * Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen
  * Duke University, Microsoft Dynamics 365 AI, Microsoft Research
* **In-Context Learning with Iterative Demonstration Selection** [EMNLP 2024] [pdf](https://aclanthology.org/2024.findings-emnlp.438.pdf) 
  * Chengwei Qin, Aston Zhang, Chen Chen, Anirudh Dagar, Wenming Ye
  * Nanyang Technological University, Amazon Web Services
* **Dr.ICL: Demonstration-Retrieved In-context Learning** [arxiv 2023.5.23] [pdf](https://arxiv.org/pdf/2305.14128) 
  * Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, Vincent Y Zhao
  * Arizona State University, Google Research
* **Learning To Retrieve Prompts for In-Context Learning** [arxiv 2022.5.8] [pdf](https://arxiv.org/pdf/2112.08633) 
  * Ohad Rubin, Jonathan Herzig, Jonathan Berant
  * The Blavatnik School of Computer Science, Tel Aviv University
* **Unified Demonstration Retriever for In-Context Learning** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.256.pdf) [github](https://github.com/KaiLv69/UDR)
  * Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, Xipeng Qiu
  * Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, School of Computer Science, Fudan University, East China Normal University, Pingan Health Tech
* **Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers** [ACL 2023] [pdf](https://aclanthology.org/2023.findings-acl.247.pdf) [github](https://github.com/microsoft/LMOps/tree/main/understand_icl)
  * Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu Wei
  * MOE Key Lab of Computational Linguistics, Peking University, Tsinghua University, Microsoft Research
* **Finding Support Examples for In-Context Learning** [EMNLP 2023] [pdf](https://aclanthology.org/2023.findings-emnlp.411.pdf) [github](https://github.com/LeeSureman/ICL_Support_Example)
  * Xiaonan Li, Xipeng Qiu
  * School of Computer Science, Fudan University, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
* **Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning** [arxiv 2024.2.12] [pdf](https://arxiv.org/pdf/2301.11916) [github](https://github.com/WANGXinyiLinda/concept-based-demonstration-selection)
  * Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang
  * Department of Computer Science, University of California, Santa Barbara, Department of Cognitive Sciences, University of California, Irvine
* **Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity** [ACL 2022] [pdf](https://aclanthology.org/2022.acl-long.556.pdf) 
  * Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp
  * University College London, Mishcon de Reya LLP
* **Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.79.pdf) 
  * Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong
  * Shanghai Artificial Intelligence Laboratory, Xiamen University, The University of Hong Kong
* **RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning** [arxiv 2024.4.16] [pdf](https://arxiv.org/pdf/2305.14502) 
  * Alexander Scarlatos, Andrew Lan
  * University of Massachusetts Amherst
* **Automatic Chain of Thought Prompting in Large Language Models** [arxiv 2022.10.7] [pdf](https://arxiv.org/pdf/2210.03493) [github](https://github.com/amazon-science/auto-cot)
  * Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola
  * Shanghai Jiao Tong University, Amazon Web Services
* **Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations** [EMNLP 2023] [pdf](https://aclanthology.org/2023.emnlp-main.968.pdf) [github](https://github.com/ntunlplab/Self-ICL)
  * Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, Hsin-Hsi Chen
  * National Taiwan University, Taiwan
* **Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.129.pdf) 
  * Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, Hannaneh Hajishirzi
  * University of Washington, Allen Institute for AI
* **Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator** [arxiv 2022.6.16] [pdf](https://arxiv.org/pdf/2206.08082) 
  * Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, Sang-goo Lee
  * Seoul National University, Hanyang University, NAVER AI Lab, NAVER CLOVA
* **Demonstration Augmentation for Zero-shot In-context Learning** [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.846.pdf) [github](https://github.com/yisunlp/DAIL)
  * Yi Su, Yunpeng Tai, Yixin Ji, Juntao Li, Bowen Yan, Min Zhang
  * School of Computer Science and Technology, Soochow University, Department of Computer Science and Technology, Tsinghua University
### Editing the Representation
* **Plug and Play Language Models: A Simple Approach to Controlled Text Generation** [arxiv 2022.3.3] [pdf](https://arxiv.org/pdf/1912.02164)
  * Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu
  * CMS, Caltech, HKUST, Uber AI
* **Steering Language Models With Activation Engineering** [arxiv 2024.10.10] [pdf](https://arxiv.org/pdf/2308.10248) 
  * Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, Monte MacDiarmid
  * MIRI, University of Bristol, MATS
* **Improving Instruction-Following in Language Models through Activation Steering** [arxiv 2024.10.15] [pdf](https://arxiv.org/pdf/2410.12877) 
  * Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, Besmira Nushi
  * ETH Zurich, Microsoft Research
* **Inference-Time Intervention: Eliciting Truthful Answers from a Language Model** [arxiv 2024.6.26] [pdf](https://arxiv.org/pdf/2306.03341) [github](https://github.com/likenneth/honest_llama) 
  * Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg
  * Harvard University
* **Refusal in Language Models Is Mediated by a Single Direction** [arxiv 2024.10.30] [pdf](https://arxiv.org/pdf/2406.11717) [github](https://github.com/andyrdt/refusal_direction)
  * Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, Neel Nanda
  * ETH Zürich, University of Maryland, MIT
* **In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering** [arxiv 2024.2.13] [pdf](https://arxiv.org/pdf/2311.06668) [github](https://github.com/shengliu66/ICV)
  * Sheng Liu, Haotian Ye, Lei Xing, James Zou
  * Stanford University
* **Investigating Bias Representations in Llama 2 Chat via Activation Steering** [arxiv 2024.2.1] [pdf](https://arxiv.org/pdf/2402.00402) 
  * Dawn Lu, Nina Rimsky
  * UC Berkeley, SPAR
* **Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization** [arxiv 2024.7.29] [pdf](https://arxiv.org/pdf/2406.00045) [github](https://github.com/CaoYuanpu/BiPO)
  * Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, Jinghui Chen
  * The Pennsylvania State University
* **Spectral Editing of Activations for Large Language Model Alignment** [arxiv 2024.11.3] [pdf](https://arxiv.org/pdf/2405.09719) [github](https://github.com/yfqiu-nlp/sea-llm)
  * Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, Shay B. Cohen
  * Institute for Language, Cognition and Computation, University of Edinburgh, Language Technology Lab, University of Cambridge
* **Multi-property Steering of Large Language Models with Dynamic Activation Composition** [BlackboxNLP 2024] [pdf](https://aclanthology.org/2024.blackboxnlp-1.34.pdf) [github](https://github.com/DanielSc4/Dynamic-Activation-Composition)
  * Daniel Scalena, Gabriele Sarti, Malvina Nissim
  * University of Milano-Bicocca CLCG, University of Groningen
### Calibrating the Output
* **Generalization through Memorization: Nearest Neighbor Language Models** [arxiv 2020.2.15] [pdf](https://arxiv.org/pdf/1911.00172)
  * Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis
  * Stanford University, Facebook AI Research
* **Nearest Neighbor Machine Translation** [arxiv 2021.7.22] [pdf](https://arxiv.org/pdf/2010.00710) [github](https://github.com/facebookresearch/fairseq/tree/main/examples/translation)
  * Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis
  * Stanford University, Facebook AI Research
* **Efficient Cluster-Based k-Nearest-Neighbor Machine Translation** [ACL 2022] [pdf](https://aclanthology.org/2022.acl-long.154.pdf) [github](https://github.com/tjunlp-lab/PCKMT)
  * Dexin Wang, Kai Fan, Boxing Chen, Deyi Xiong
  * Tianjin University
* **What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation** [arxiv 2022.12.20] [pdf](https://arxiv.org/pdf/2211.04052) [github](https://github.com/facebookresearch/fairseq/tree/main/examples/wmt19)
  * Wenhao Zhu, Shujian Huang, Yunzhe Lv, Xin Zheng, Jiajun Chen
  * National Key Laboratory for Novel Software Technology, Nanjing University, China, Collaborative Innovation Center of Novel Software Technology and Industrialization
* **Efficient Domain Adaptation for Non-Autoregressive Machine Translation** [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.810.pdf) [github](https://github.com/Moriarty0923/BIKNN)
  * Pedro Henrique Martins, Zita Marinho, André F. T. Martins
  * Instituto de Telecomunicações, DeepMind, Institute of Systems and Robotics, LUMLIS (Lisbon ELLIS Unit), Instituto Superior Técnico, Unbabel Lisbon, Portugal
* **kNN-NER: Named Entity Recognition with Nearest Neighbor Search** [arxiv 2022.3.31] [pdf](https://arxiv.org/pdf/2203.17103) [github](https://github.com/ShannonAI/KNN-NER.)
  * Shuhe Wang, Xiaoya Li, Yuxian Meng, Tianwei Zhang, Rongbin Ouyang, Jiwei Li, Guoyin Wang
  * Shannon.AI, Peking University, Nanyang Technological University, Zhejiang University, Amazon Alexa AI
* **kNN-CM: A Non-parametric Inference-Phase Adaptation of Parametric Text Classifiers** [EMNLP 2023] [pdf](https://aclanthology.org/2023.findings-emnlp.903.pdf) [github](https://github.com/Bhardwaj-Rishabh/kNN-CM)
  * Rishabh Bhardwaj, Yingting Li, Navonil Majumder, Bo Cheng, Soujanya Poria
  * Singapore University of Technology and Design, Singapore, Beijing University of Posts and Telecommunications
* **AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation** [arxiv 2023.5.10] [pdf](https://arxiv.org/pdf/2304.12566) [github](https://github.com/yfzhang114/AdaNPC)
  * Yi-Fan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan
  * Work done during an internship at Alibaba Group, School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS), MAIS, CRIPAC, CASIA, Machine Intelligence Technology, Alibaba Group, Department of Mathematics at Princeton University, Center for Machine Learning Research, Peking University, Work done at Alibaba Group, and now affiliated with Meta., Nanjing University





## Test-time Reasoning

### Reapted sampling

* **Competition-level code generation with alphacode** [arxiv 2022.2] [pdf](https://arxiv.org/pdf/2203.07814) [github](https://github.com/google-deepmind/code_contests)
  * Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals
  * Google-DeepMind
* **Code Llama: Open Foundation Models for Code** [arxiv 2023.8] [pdf](https://arxiv.org/pdf/2308.12950) [github](https://github.com/meta-llama/codellama)
  * Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve
  * Meta AI
* **More Agents Is All You Need** [arxiv 2024.2] [pdf](https://arxiv.org/pdf/2402.05120) [github](https://github.com/MoreAgentsIsAllYouNeed/AgentForest)
  * Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye
  * Tencent
* **Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios** [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.230.pdf)
  * Lei Lin, Jiayi Fu, Pengli Liu, Qingyang Li, Yan Gong, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai
  * Kuaishou Technology, Beijing, China   School of Computer Science and Engineering, Northeastern University, Shenyang, China 
* **Self-Consistency Improves Chain of Thought Reasoning in Language Models** [ICLR 2023] [pdf](https://openreview.net/pdf?id=1PL1NIMMrw)
  * Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou
  * Google Research, Brain Team

* **Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning **[arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.12608) [github](https://github.com/declare-lab/prove)
  * Vernon Y.H. Toh, Deepanway Ghosal, Soujanya Poria
  * Singapore University of Technology and Design

* **Learning to summarize with human feedback **[NeurIPS 2020] [pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf)
  * Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano
  * OpenAI

* **Training Verifiers to Solve Math Word Problems **[arxiv 2021.10] [pdf](https://arxiv.org/pdf/2110.14168)
  * Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman
  *  OpenAI

* **WebGPT: Browser-assisted question-answering with human feedback** [arxiv 2021.12] [pdf](https://arxiv.org/pdf/2112.09332)
  * Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman
  *  OpenAI

* **Making Language Models Better Reasoners with Step-Aware Verifier** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.291.pdf) [github](https://github.com/microsoft/DiVeRSe)
  * Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen
  *  National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University   Microsoft Corporation

6. Accelerating Best-of-N via Speculative Rejection [ICML 2024] [pdf](https://openreview.net/pdf?id=dRp8tAIPhj)
7. TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.16033)
8. Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.02725)
9. Advancing LLM Reasoning Generalists with Preference Trees [arxiv 2024.4] [pdf](https://arxiv.org/pdf/2404.02078)
10. Solving math word problems with process- and outcome-based feedback [arxiv 2022.11] [pdf](https://arxiv.org/pdf/2211.14275)
11. Let's Verify Step by Step [ICLR 2024] [pdf](https://openreview.net/pdf?id=v8L0pN6EOi)
12. Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations [ACL 2024] [pdf](https://aclanthology.org/2024.acl-long.510.pdf)
13. Improve Mathematical Reasoning in Language Models by Automated Process Supervision [arxiv 20224.6] [pdf](https://arxiv.org/pdf/2406.06592)
14. Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning [arxiv 20224.10] [pdf](https://arxiv.org/pdf/2410.08146)

### Self-correction

1. Reflexion: Language Agents with Verbal Reinforcement Learning [arxiv 2023.3] [pdf](https://arxiv.org/pdf/2303.11366)
2. Interscript: A dataset for interactive learning of scripts through error feedback [arxiv 2021.12] [pdf](https://arxiv.org/pdf/2112.07867)
3. NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction [ACL 2021] [pdf](https://aclanthology.org/2021.naacl-main.444.pdf)
4. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback [ACL 2022] [pdf](https://aclanthology.org/2022.findings-naacl.26.pdf)
5. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing [ICLR 2024] [pdf](https://openreview.net/pdf?id=Sx038qxjek)
6. Teaching Large Language Models to Self-Debug [ICLR 2024] [pdf](https://openreview.net/pdf?id=KuPixIqPiq)
7. 
