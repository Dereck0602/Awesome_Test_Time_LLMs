<div align="center">
<h1>Awesome Test-time Computing</h1>
</div>

## Test-time Adaptation
### Updating the Model
1. **Test-Time Training with Self-Supervision for Generalization under Distribution Shifts** [arxiv 2019.9.29] [pdf](https://arxiv.org/pdf/1909.13231)
2. **MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption** [arxiv 2022.1.20] [pdf](https://arxiv.org/pdf/2103.16201)
3. **Test-Time Training with Masked Autoencoders** [arxiv 2022.9.15] [pdf](https://arxiv.org/pdf/2209.07522)
4. **TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?** [NeurIPS 2021] [pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/b618c3210e934362ac261db280128c22-Paper.pdf)
5. **Efficient Test-Time Prompt Tuning for Vision-Language Models** [arxiv 2024.8.11] [pdf](https://arxiv.org/pdf/2408.05775)
6. **Tent: Fully Test-time Adaptation by Entropy Minimization** [arxiv 2021.3.18] [pdf](https://arxiv.org/pdf/2006.10726)
7. **MEMO: Test Time Robustness via Adaptation and Augmentation** [arxiv 2022.10.10] [pdf](https://arxiv.org/pdf/2110.09506)
8. **The Entropy Enigma: Success and Failure of Entropy Minimization** [arxiv 2024.5.12] [pdf](https://arxiv.org/pdf/2405.05012)
9. **On Pitfalls of Test-Time Adaptation** [arxiv 2023.6.6] [pdf](https://arxiv.org/pdf/2306.03536)
10. **Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering** [EMNLP 2023] [pdf](https://aclanthology.org/2023.emnlp-main.803.pdf)
11. **Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization** [arxiv 2024.1.11] [pdf](https://arxiv.org/pdf/2311.01459)
12. **Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach** [arxiv 2024.8.14] [pdf](https://arxiv.org/pdf/2408.07511)
13. **Simulating Bandit Learning from User Feedback for Extractive Question Answering** [ACL 2022] [pdf](https://aclanthology.org/2022.acl-long.355.pdf)
14. **Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment** [ACL 2022] [pdf](https://aclanthology.org/2022.findings-acl.75.pdf)
15. **Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.47.pdf)
16. **COMET: A Neural Framework for MT Evaluation** [EMNLP 2020] [pdf](https://aclanthology.org/2020.emnlp-main.213.pdf)
17. **Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models** [arxiv 2023.5.29] [pdf](https://arxiv.org/pdf/2305.18010)
18. **Improving robustness against common corruptions by covariate shift adaptation** [arxiv 2020.10.23] [pdf](https://arxiv.org/pdf/2006.16971)
19. **Selective Annotation Makes Language Models Better Few-Shot Learners** [arxiv 2022.9.5] [pdf](https://arxiv.org/pdf/2209.01975)
20. **Test-Time Adaptation with Perturbation Consistency Learning** [arxiv 2023.4.25] [pdf](https://arxiv.org/pdf/2304.12764)
21. **Test-Time Prompt Adaptation for Vision-Language Models** [NeurIPS 2023] [pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/cdd0640218a27e9e2c0e52e324e25db0-Paper-Conference.pdf)
22. **Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning** [arxiv 2023.8.17] [pdf](https://arxiv.org/pdf/2308.06038)
23. **Test-Time Model Adaptation with Only Forward Passes** [arxiv 2024.5.29] [pdf](https://arxiv.org/pdf/2404.01650)
24. **Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot Generalization of Vision-Language Models** [arxiv 2024.7.22] [pdf](https://arxiv.org/pdf/2407.15913)
25. **StreamAdapter: Efficient Test Time Adaptation from Contextual Streams** [arxiv 2024.11.14] [pdf](https://arxiv.org/pdf/2411.09289)
26. **Towards Stable Test-time Adaptation in Dynamic Wild World** [arxiv 2023.2.24] [pdf](https://arxiv.org/pdf/2302.12400)
27. **SoTTA: Robust Test-Time Adaptation on Noisy Data Streams** [arxiv 2023.10.16] [pdf](https://arxiv.org/pdf/2310.10074)
28. **Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time** [arxiv 2022.7.1] [pdf](https://arxiv.org/pdf/2203.05482)
29. **Robust Question Answering against Distribution Shifts with Test-Time Adaption: An Empirical Study** [EMNLP 2022] [pdf](https://aclanthology.org/2022.findings-emnlp.460.pdf)
### Modifying the Input
1. **What Makes Good In-Context Examples for GPT-3?** [DeeLIO 2022] [pdf](https://aclanthology.org/2022.deelio-1.10.pdf)
2. **In-Context Learning with Iterative Demonstration Selection** [EMNLP 2024] [pdf](https://aclanthology.org/2024.findings-emnlp.438.pdf)
3. **Dr.ICL: Demonstration-Retrieved In-context Learning** [arxiv 2023.5.23] [pdf](https://arxiv.org/pdf/2305.14128)
4. **Learning To Retrieve Prompts for In-Context Learning** [arxiv 2022.5.8] [pdf](https://arxiv.org/pdf/2112.08633)
5. **Unified Demonstration Retriever for In-Context Learning** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.256.pdf)
6. **Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers** [ACL 2023] [pdf](https://aclanthology.org/2023.findings-acl.247.pdf)
7. **Finding Support Examples for In-Context Learning** [EMNLP 2023] [pdf](https://aclanthology.org/2023.findings-emnlp.411.pdf)
8. **Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning** [arxiv 2024.2.12] [pdf](https://arxiv.org/pdf/2301.11916)
9. **Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity** [ACL 2022] [pdf](https://aclanthology.org/2022.acl-long.556.pdf)
10. **Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.79.pdf)
11. **RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning** [arxiv 2024.4.16] [pdf](https://arxiv.org/pdf/2305.14502)
12. **Automatic Chain of Thought Prompting in Large Language Models** [arxiv 2022.10.7] [pdf](https://arxiv.org/pdf/2210.03493)
13. **Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations** [EMNLP 2023] [pdf](https://aclanthology.org/2023.emnlp-main.968.pdf)
14. **Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.129.pdf)
15. **Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator** [arxiv 2022.6.16] [pdf](https://arxiv.org/pdf/2206.08082)
16. **Demonstration Augmentation for Zero-shot In-context Learning** [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.846.pdf)
### Editing the Representation
1. **Plug and Play Language Models: A Simple Approach to Controlled Text Generation** [arxiv 2022.3.3] [pdf](https://arxiv.org/pdf/1912.02164)
2. **Steering Language Models With Activation Engineering** [arxiv 2024.10.10] [pdf](https://arxiv.org/pdf/2308.10248)
3. **Improving Instruction-Following in Language Models through Activation Steering** [arxiv 2024.10.15] [pdf](https://arxiv.org/pdf/2410.12877)
4. **Inference-Time Intervention: Eliciting Truthful Answers from a Language Model** [arxiv 2024.6.26] [pdf](https://arxiv.org/pdf/2306.03341)
5. **Refusal in Language Models Is Mediated by a Single Direction** [arxiv 2024.10.30] [pdf](https://arxiv.org/pdf/2406.11717)
6. **In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering** [arxiv 2024.2.13] [pdf](https://arxiv.org/pdf/2311.06668)
7. **Investigating Bias Representations in Llama 2 Chat via Activation Steering** [arxiv 2024.2.1] [pdf](https://arxiv.org/pdf/2402.00402)
8. **Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization** [arxiv 2024.7.29] [pdf](https://arxiv.org/pdf/2406.00045)
9. **Spectral Editing of Activations for Large Language Model Alignment** [arxiv 2024.11.3] [pdf](https://arxiv.org/pdf/2405.09719)
10. **Multi-property Steering of Large Language Models with Dynamic Activation Composition** [BlackboxNLP 2024] [pdf](https://aclanthology.org/2024.blackboxnlp-1.34.pdf)
### Calibrating the Output
1. **Generalization through Memorization: Nearest Neighbor Language Models** [arxiv 2020.2.15] [pdf](https://arxiv.org/pdf/1911.00172)
2. **Nearest Neighbor Machine Translation** [arxiv 2021.7.22] [pdf](https://arxiv.org/pdf/2010.00710)
3. **Efficient Cluster-Based k-Nearest-Neighbor Machine Translation** [ACL 2022] [pdf](https://aclanthology.org/2022.acl-long.154.pdf)
4. **What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation** [arxiv 2022.12.20] [pdf](https://arxiv.org/pdf/2211.04052)
5. **Efficient Domain Adaptation for Non-Autoregressive Machine Translation** [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.810.pdf)
6. **kNN-NER: Named Entity Recognition with Nearest Neighbor Search** [arxiv 2022.3.31] [pdf](https://arxiv.org/pdf/2203.17103)
7. **kNN-CM: A Non-parametric Inference-Phase Adaptation of Parametric Text Classifiers** [EMNLP 2023] [pdf](https://aclanthology.org/2023.findings-emnlp.903.pdf)
8. **AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation** [arxiv 2023.5.10] [pdf](https://arxiv.org/pdf/2304.12566)



## Test-time Reasoning
### Reapted sampling
1. Competition-level code generation with alphacode [arxiv 2022.2] [pdf](https://arxiv.org/pdf/2203.07814)
2. Code Llama: Open Foundation Models for Code [arxiv 2023.8] [pdf](https://arxiv.org/pdf/2308.12950)
3. More Agents Is All You Need [arxiv 2024.2] [pdf](https://arxiv.org/pdf/2402.05120)
4. Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.230.pdf)
5. Self-Consistency Improves Chain of Thought Reasoning in Language Models [ICLR 2023] [pdf](https://openreview.net/pdf?id=1PL1NIMMrw)
6. Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.12608)
7. Learning to summarize with human feedback [NeurIPS 2020] [pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf)
8. Training Verifiers to Solve Math Word Problems [arxiv 2021.10] [pdf](https://arxiv.org/pdf/2110.14168)
9. WebGPT: Browser-assisted question-answering with human feedback [arxiv 2021.12] [pdf](https://arxiv.org/pdf/2112.09332)
10. Making Language Models Better Reasoners with Step-Aware Verifier [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.291.pdf)
11. Accelerating Best-of-N via Speculative Rejection [ICML 2024] [pdf](https://openreview.net/pdf?id=dRp8tAIPhj)
12. TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.16033)
13. Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.02725)
14. Advancing LLM Reasoning Generalists with Preference Trees [arxiv 2024.4] [pdf](https://arxiv.org/pdf/2404.02078)
15. Solving math word problems with process- and outcome-based feedback [arxiv 2022.11] [pdf](https://arxiv.org/pdf/2211.14275)
16. Let's Verify Step by Step [ICLR 2024] [pdf](https://openreview.net/pdf?id=v8L0pN6EOi)
17. Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations [ACL 2024] [pdf](https://aclanthology.org/2024.acl-long.510.pdf)
18. Improve Mathematical Reasoning in Language Models by Automated Process Supervision [arxiv 20224.6] [pdf](https://arxiv.org/pdf/2406.06592)
19. Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning [arxiv 20224.10] [pdf](https://arxiv.org/pdf/2410.08146)

### Self-correction
1. Reflexion: Language Agents with Verbal Reinforcement Learning [arxiv 2023.3] [pdf](https://arxiv.org/pdf/2303.11366)
2. Interscript: A dataset for interactive learning of scripts through error feedback [arxiv 2021.12] [pdf](https://arxiv.org/pdf/2112.07867)
3. NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction [ACL 2021] [pdf](https://aclanthology.org/2021.naacl-main.444.pdf)
4. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback [ACL 2022] [pdf](https://aclanthology.org/2022.findings-naacl.26.pdf)
5. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing [ICLR 2024] [pdf](https://openreview.net/pdf?id=Sx038qxjek)
6. Teaching Large Language Models to Self-Debug [ICLR 2024] [pdf](https://openreview.net/pdf?id=KuPixIqPiq)
7. 
