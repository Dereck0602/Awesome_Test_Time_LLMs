<div align="center">
<h1>Awesome Test-time Computing</h1>
</div>

## Test-time Adaptation
### Updating the Model
* **Test-Time Training with Self-Supervision for Generalization under Distribution Shifts** [arxiv 2019.9.29] [pdf](https://arxiv.org/pdf/1909.13231)
  * Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, Moritz Hardt
  * University of California, Berkeley
* **MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption** [arxiv 2022.1.20] [pdf](https://arxiv.org/pdf/2103.16201) [github](https://github.com/AlexanderBartler/MT3)
  * Alexander Bartler, Andre Bühler, Felix Wiewel, Mario Döbler, Bin Yang
  * Institute of Signal Processing and System Theory, University of Stuttgart, Germany
* **Test-Time Training with Masked Autoencoders** [arxiv 2022.9.15] [pdf](https://arxiv.org/pdf/2209.07522)
  * Yossi Gandelsman, Yu Sun, , Xinlei Chen, Alexei A. Efros
  * UC Berkeley, Meta AI
* **TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?** [NeurIPS 2021] [pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/b618c3210e934362ac261db280128c22-Paper.pdf) [github](https://github.com/vita-epfl/ttt-plus-plus)
  * Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, Alexandre Alahi
  * École Polytechnique Fédérale de Lausanne
* **Efficient Test-Time Prompt Tuning for Vision-Language Models** [arxiv 2024.8.11] [pdf](https://arxiv.org/pdf/2408.05775)
  * Yuhan Zhu, Guozhen Zhang, Chen Xu, Haocheng Shen, Xiaoxin Chen, Gangshan Wu, Limin Wang
  * State Key Laboratory for Novel Software Technology, Nanjing University, vivo AI Lab 3Shanghai AI Laboratory
* **Tent: Fully Test-time Adaptation by Entropy Minimization** [arxiv 2021.3.18] [pdf](https://arxiv.org/pdf/2006.10726) [github](https://github.com/vita-epfl/ttt-plus-plus)
  * Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, Trevor Darrell
  * UC Berkeley, Adobe Research
* **MEMO: Test Time Robustness via Adaptation and Augmentation** [arxiv 2022.10.10] [pdf](https://arxiv.org/pdf/2110.09506) [github](https://github.com/zhangmarvin/memo)
  * Marvin Zhang, Sergey Levine, Chelsea Finn
  * UC Berkeley, Stanford University
* **The Entropy Enigma: Success and Failure of Entropy Minimization** [arxiv 2024.5.12] [pdf](https://arxiv.org/pdf/2405.05012) [github](https://github.com/oripress/EntropyEnigma)
  * Ori Press, Ravid Shwartz-Ziv, Yann LeCun, Matthias Bethge
  * University of Tubingen, Tubingen AI Center, New York University, Meta AI, FAIR.
* **On Pitfalls of Test-Time Adaptation** [arxiv 2023.6.6] [pdf](https://arxiv.org/pdf/2306.03536) [github](https://github.com/lins-lab/ttab)
  * Hao Zhao, Yuejiang Liu, Alexandre Alahi, Tao Lin
  * École Polytechnique Fédérale de Lausanne (EPFL), Research Center for Industries of the Future, Westlake University, School of Engineering, Westlake University
* **Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering** [EMNLP 2023] [pdf](https://aclanthology.org/2023.emnlp-main.803.pdf) [github](https://github.com/yisunlp/Anti-CF)
  * Yi Su, Yixin Ji, Juntao Li, Hai Ye, Min Zhang
  * Institute of Computer Science and Technology, Soochow University, China, Department of Computer Science, National University of Singapore
* **Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization** [arxiv 2024.1.11] [pdf](https://arxiv.org/pdf/2311.01459) 
  * Jameel Hassan, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Shahbaz Khan, Salman Khan
  * Mohamed Bin Zayed University of AI, Linköping University, Australian National University
* **Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach** [arxiv 2024.8.14] [pdf](https://arxiv.org/pdf/2408.07511) [github](https://github.com/yarinbar/poem)
  * Yarin Bar, Shalev Shaer, Yaniv Romano
  * Department of Computer Science, Technion IIT, Israel, Department of Electrical and Computer Engineering, Technion IIT, Israel
* **Simulating Bandit Learning from User Feedback for Extractive Question Answering** [ACL 2022] [pdf](https://aclanthology.org/2022.acl-long.355.pdf) [github](https://github.com/lil-lab/bandit-qa)
  * Ge Gao, Eunsol Choi, Yoav Artzi
  * Department of Computer Science and Cornell Tech, Cornell University, Department of Computer Science, The University of Texas at Austin
* **Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment** [ACL 2022] [pdf](https://aclanthology.org/2022.findings-acl.75.pdf) 
  * Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie C.K. Cheung, Siva Reddy
  * Mila, McGill University, University of California, Los Angeles
* **Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.47.pdf) [github](https://github.com/lil-lab/bandit-qa)
  * Runzhe Zhan, Xuebo Liu, Derek F. Wong, Cuilian Zhang, Lidia S. Chao, Min Zhang
  * NLPCT Lab, Department of Computer and Information Science, University of Macau, Institute of Computing, Harbin Institute of Technology, Shenzhen, China
* **COMET: A Neural Framework for MT Evaluation** [EMNLP 2020] [pdf](https://aclanthology.org/2020.emnlp-main.213.pdf) 
  * Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie
  * Unbabel AI
* **Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models** [arxiv 2023.5.29] [pdf](https://arxiv.org/pdf/2305.18010) [github](https://github.com/lil-lab/bandit-qa)
  * Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yi Yang
  * ReLER Lab, AAII, University of Technology Sydney, ReLER Lab, CCAI, Zhejiang University, Stanford University, Baidu Inc.
* **Improving robustness against common corruptions by covariate shift adaptation** [arxiv 2020.10.23] [pdf](https://arxiv.org/pdf/2006.16971) 
  * Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, Matthias Bethge
  * University of Tübingen & IMPRS-IS, LMU Munich, University of Tübingen
* **Selective Annotation Makes Language Models Better Few-Shot Learners** [arxiv 2022.9.5] [pdf](https://arxiv.org/pdf/2209.01975) [github](https://github.com/xlang-ai/icl-selective-annotation)
  * Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu
  * The University of Hong Kong, University of Washington, Allen Institute for AI, Carnegie Mellon University FPenn State University, Meta AI
* **Test-Time Adaptation with Perturbation Consistency Learning** [arxiv 2023.4.25] [pdf](https://arxiv.org/pdf/2304.12764) 
  * Yi Su, Yixin Ji, Juntao Li, Hai Ye, Min Zhang
  * Equal contribution, Institute of Computer Science and Technology, Soochow University, China Department of Computer Science, National University of Singapore, Singapore.
* **Test-Time Prompt Adaptation for Vision-Language Models** [NeurIPS 2023] [pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/cdd0640218a27e9e2c0e52e324e25db0-Paper-Conference.pdf) 
  * Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, Chaowei Xiao
  * University of Maryland, NVIDIA, Caltech, Arizona State University
* **Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning** [arxiv 2023.8.17] [pdf](https://arxiv.org/pdf/2308.06038) [github](https://github.com/chunmeifeng/DiffTPT)
  * Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, Wangmeng Zuo
  * Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), UAE, Australian National University, Canberra ACT, Australia, Harbin Institute of Technology, Harbin, China
* **Test-Time Model Adaptation with Only Forward Passes** [arxiv 2024.5.29] [pdf](https://arxiv.org/pdf/2404.01650) [github](https://github.com/mr-eggplant/FOA)
  * Shuaicheng Niu, Chunyan Miao, Guohao Chen, Pengcheng Wu, Peilin Zhao
  * College of Computing and Data Science, Nanyang Technological University, Singapore, Joint NTU-WeBank Research Centre onFintech, Singapore, Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Singapore, Tencent AI Lab, Shenzhen, China.
* **Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot Generalization of Vision-Language Models** [arxiv 2024.7.22] [pdf](https://arxiv.org/pdf/2407.15913) [github](https://github.com/Razaimam45/TTL-Test-Time-Low-Rank-Adaptation)
  * Raza Imam, Hanan Gani, Muhammad Huzaifa, Karthik Nandakumar
  * Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), UAE
* **StreamAdapter: Efficient Test Time Adaptation from Contextual Streams** [arxiv 2024.11.14] [pdf](https://arxiv.org/pdf/2411.09289) 
  * Dilxat Muhtar, Yelong Shen, Yaming Yang, Xiaodong Liu, Yadong Lu, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Xueliang Zhang, Jianfeng Gao, Weizhu Chen, Qi Zhang
  * Nanjing University, Microsoft
* **Towards Stable Test-time Adaptation in Dynamic Wild World** [arxiv 2023.2.24] [pdf](https://arxiv.org/pdf/2302.12400) [github](https://github.com/mr-eggplant/SAR)
  * Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, Mingkui Tan
  * South China University of Technology, Tencent AI Lab, National University of Singapore, Key Laboratory of Big Data and Intelligent Robot, Ministry of Education4 Pazhou Laboratory
* **SoTTA: Robust Test-Time Adaptation on Noisy Data Streams** [arxiv 2023.10.16] [pdf](https://arxiv.org/pdf/2310.10074) [github](https://github.com/taeckyung/SoTTA)
  * Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chottananurak, Sung-Ju Lee
  * Nokia Bell Labs, KAIST
* **Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time** [arxiv 2022.7.1] [pdf](https://arxiv.org/pdf/2203.05482) [github](https://github.com/mlfoundations/model-soups)
  * Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt
  * University of Washington, Columbia University, Google Research, Brain Team, Meta AI Research, TelAviv University
* **Robust Question Answering against Distribution Shifts with Test-Time Adaption: An Empirical Study** [EMNLP 2022] [pdf](https://aclanthology.org/2022.findings-emnlp.460.pdf) [github](https://github.com/oceanypt/coldqa-tta)
  * Hai Ye, Yuyang Ding, Juntao Li, Hwee Tou Ng
  * Department of Computer Science, National University of Singapore, Soochow University, China
### Modifying the Input
* **What Makes Good In-Context Examples for GPT-3?** [DeeLIO 2022] [pdf](https://aclanthology.org/2022.deelio-1.10.pdf) 
  * Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen
  * Duke University, Microsoft Dynamics 365 AI, Microsoft Research
* **In-Context Learning with Iterative Demonstration Selection** [EMNLP 2024] [pdf](https://aclanthology.org/2024.findings-emnlp.438.pdf) 
  * Chengwei Qin, Aston Zhang, Chen Chen, Anirudh Dagar, Wenming Ye
  * Nanyang Technological University, Amazon Web Services
* **Dr.ICL: Demonstration-Retrieved In-context Learning** [arxiv 2023.5.23] [pdf](https://arxiv.org/pdf/2305.14128) 
  * Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, Vincent Y Zhao
  * Arizona State University, Google Research
* **Learning To Retrieve Prompts for In-Context Learning** [arxiv 2022.5.8] [pdf](https://arxiv.org/pdf/2112.08633) 
  * Ohad Rubin, Jonathan Herzig, Jonathan Berant
  * The Blavatnik School of Computer Science, Tel Aviv University
* **Unified Demonstration Retriever for In-Context Learning** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.256.pdf) [github](https://github.com/KaiLv69/UDR)
  * Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, Xipeng Qiu
  * Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, School of Computer Science, Fudan University, East China Normal University, Pingan Health Tech
* **Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers** [ACL 2023] [pdf](https://aclanthology.org/2023.findings-acl.247.pdf) [github](https://github.com/microsoft/LMOps/tree/main/understand_icl)
  * Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu Wei
  * MOE Key Lab of Computational Linguistics, Peking University, Tsinghua University, Microsoft Research
* **Finding Support Examples for In-Context Learning** [EMNLP 2023] [pdf](https://aclanthology.org/2023.findings-emnlp.411.pdf) [github](https://github.com/LeeSureman/ICL_Support_Example)
  * Xiaonan Li, Xipeng Qiu
  * School of Computer Science, Fudan University, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
* **Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning** [arxiv 2024.2.12] [pdf](https://arxiv.org/pdf/2301.11916) [github](https://github.com/WANGXinyiLinda/concept-based-demonstration-selection)
  * Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang
  * Department of Computer Science, University of California, Santa Barbara, Department of Cognitive Sciences, University of California, Irvine
* **Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity** [ACL 2022] [pdf](https://aclanthology.org/2022.acl-long.556.pdf) 
  * Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp
  * University College London, Mishcon de Reya LLP
* **Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.79.pdf) 
  * Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong
  * Shanghai Artificial Intelligence Laboratory, Xiamen University, The University of Hong Kong
* **RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning** [arxiv 2024.4.16] [pdf](https://arxiv.org/pdf/2305.14502) 
  * Alexander Scarlatos, Andrew Lan
  * University of Massachusetts Amherst
* **Automatic Chain of Thought Prompting in Large Language Models** [arxiv 2022.10.7] [pdf](https://arxiv.org/pdf/2210.03493) [github](https://github.com/amazon-science/auto-cot)
  * Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola
  * Shanghai Jiao Tong University, Amazon Web Services
* **Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations** [EMNLP 2023] [pdf](https://aclanthology.org/2023.emnlp-main.968.pdf) [github](https://github.com/ntunlplab/Self-ICL)
  * Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, Hsin-Hsi Chen
  * National Taiwan University, Taiwan
* **Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.129.pdf) 
  * Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, Hannaneh Hajishirzi
  * University of Washington, Allen Institute for AI
* **Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator** [arxiv 2022.6.16] [pdf](https://arxiv.org/pdf/2206.08082) 
  * Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, Sang-goo Lee
  * Seoul National University, Hanyang University, NAVER AI Lab, NAVER CLOVA
* **Demonstration Augmentation for Zero-shot In-context Learning** [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.846.pdf) [github](https://github.com/yisunlp/DAIL)
  * Yi Su, Yunpeng Tai, Yixin Ji, Juntao Li, Bowen Yan, Min Zhang
  * School of Computer Science and Technology, Soochow University, Department of Computer Science and Technology, Tsinghua University
### Editing the Representation
* **Plug and Play Language Models: A Simple Approach to Controlled Text Generation** [arxiv 2022.3.3] [pdf](https://arxiv.org/pdf/1912.02164)
  * Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu
  * CMS, Caltech, HKUST, Uber AI
* **Steering Language Models With Activation Engineering** [arxiv 2024.10.10] [pdf](https://arxiv.org/pdf/2308.10248) 
  * Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, Monte MacDiarmid
  * MIRI, University of Bristol, MATS
* **Improving Instruction-Following in Language Models through Activation Steering** [arxiv 2024.10.15] [pdf](https://arxiv.org/pdf/2410.12877) 
  * Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, Besmira Nushi
  * ETH Zurich, Microsoft Research
* **Inference-Time Intervention: Eliciting Truthful Answers from a Language Model** [arxiv 2024.6.26] [pdf](https://arxiv.org/pdf/2306.03341) [github](https://github.com/likenneth/honest_llama) 
  * Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg
  * Harvard University
* **Refusal in Language Models Is Mediated by a Single Direction** [arxiv 2024.10.30] [pdf](https://arxiv.org/pdf/2406.11717) [github](https://github.com/andyrdt/refusal_direction)
  * Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, Neel Nanda
  * ETH Zürich, University of Maryland, MIT
* **In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering** [arxiv 2024.2.13] [pdf](https://arxiv.org/pdf/2311.06668) [github](https://github.com/shengliu66/ICV)
  * Sheng Liu, Haotian Ye, Lei Xing, James Zou
  * Stanford University
* **Investigating Bias Representations in Llama 2 Chat via Activation Steering** [arxiv 2024.2.1] [pdf](https://arxiv.org/pdf/2402.00402) 
  * Dawn Lu, Nina Rimsky
  * UC Berkeley, SPAR
* **Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization** [arxiv 2024.7.29] [pdf](https://arxiv.org/pdf/2406.00045) [github](https://github.com/CaoYuanpu/BiPO)
  * Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, Jinghui Chen
  * The Pennsylvania State University
* **Spectral Editing of Activations for Large Language Model Alignment** [arxiv 2024.11.3] [pdf](https://arxiv.org/pdf/2405.09719) [github](https://github.com/yfqiu-nlp/sea-llm)
  * Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, Shay B. Cohen
  * Institute for Language, Cognition and Computation, University of Edinburgh, Language Technology Lab, University of Cambridge
* **Multi-property Steering of Large Language Models with Dynamic Activation Composition** [BlackboxNLP 2024] [pdf](https://aclanthology.org/2024.blackboxnlp-1.34.pdf) [github](https://github.com/DanielSc4/Dynamic-Activation-Composition)
  * Daniel Scalena, Gabriele Sarti, Malvina Nissim
  * University of Milano-Bicocca CLCG, University of Groningen
### Calibrating the Output
* **Generalization through Memorization: Nearest Neighbor Language Models** [arxiv 2020.2.15] [pdf](https://arxiv.org/pdf/1911.00172) [github](https://github.com/urvashik/knnlm)
  * Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis
  * Stanford University, Facebook AI Research
* **Nearest Neighbor Machine Translation** [arxiv 2021.7.22] [pdf](https://arxiv.org/pdf/2010.00710) [github](https://github.com/facebookresearch/fairseq/tree/main/examples/translation)
  * Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis
  * Stanford University, Facebook AI Research
* **Efficient Cluster-Based k-Nearest-Neighbor Machine Translation** [ACL 2022] [pdf](https://aclanthology.org/2022.acl-long.154.pdf) [github](https://github.com/tjunlp-lab/PCKMT)
  * Dexin Wang, Kai Fan, Boxing Chen, Deyi Xiong
  * Tianjin University
* **What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation** [arxiv 2022.12.20] [pdf](https://arxiv.org/pdf/2211.04052) [github](https://github.com/facebookresearch/fairseq/tree/main/examples/wmt19)
  * Wenhao Zhu, Shujian Huang, Yunzhe Lv, Xin Zheng, Jiajun Chen
  * National Key Laboratory for Novel Software Technology, Nanjing University, China, Collaborative Innovation Center of Novel Software Technology and Industrialization
* **Efficient Domain Adaptation for Non-Autoregressive Machine Translation** [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.810.pdf) [github](https://github.com/Moriarty0923/BIKNN)
  * Pedro Henrique Martins, Zita Marinho, André F. T. Martins
  * Instituto de Telecomunicações, DeepMind, Institute of Systems and Robotics, LUMLIS (Lisbon ELLIS Unit), Instituto Superior Técnico, Unbabel Lisbon, Portugal
* **kNN-NER: Named Entity Recognition with Nearest Neighbor Search** [arxiv 2022.3.31] [pdf](https://arxiv.org/pdf/2203.17103) [github](https://github.com/ShannonAI/KNN-NER.)
  * Shuhe Wang, Xiaoya Li, Yuxian Meng, Tianwei Zhang, Rongbin Ouyang, Jiwei Li, Guoyin Wang
  * Shannon.AI, Peking University, Nanyang Technological University, Zhejiang University, Amazon Alexa AI
* **kNN-CM: A Non-parametric Inference-Phase Adaptation of Parametric Text Classifiers** [EMNLP 2023] [pdf](https://aclanthology.org/2023.findings-emnlp.903.pdf) [github](https://github.com/Bhardwaj-Rishabh/kNN-CM)
  * Rishabh Bhardwaj, Yingting Li, Navonil Majumder, Bo Cheng, Soujanya Poria
  * Singapore University of Technology and Design, Singapore, Beijing University of Posts and Telecommunications
* **AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation** [arxiv 2023.5.10] [pdf](https://arxiv.org/pdf/2304.12566) [github](https://github.com/yfzhang114/AdaNPC)
  * Yi-Fan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan
  * Work done during an internship at Alibaba Group, School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS), MAIS, CRIPAC, CASIA, Machine Intelligence Technology, Alibaba Group, Department of Mathematics at Princeton University, Center for Machine Learning Research, Peking University, Work done at Alibaba Group, and now affiliated with Meta., Nanjing University





## Test-time Reasoning

### Feedback Modeling

* **Training Verifiers to Solve Math Word Problems**[arxiv 2021.10] [pdf](https://arxiv.org/pdf/2110.14168)

  * Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman

  * OpenAI

* Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons [ Oxford University Press 1952] [pdf](https://www.jstor.org/stable/2334029)

  * Ralph Allan Bradley and Milton E. Terry

* **Advancing LLM Reasoning Generalists with Preference Trees** [arxiv 2024.4] [pdf](https://arxiv.org/pdf/2404.02078) [github](https://github.com/OpenBMB/Eurus)

  * Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun
  * Tsinghua University , University of Illinois Urbana-Champaign, Northeastern University , ModelBest.Inc , Renmin University of China , BUPT , Tencent

* **V-STaR: Training Verifiers for Self-Taught Reasoners** [COLM 2024] [pdf](https://openreview.net/pdf?id=stmqBSW2dV)

  * Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal
  * Mila, Universit´e de Montr´eal Microsoft Research University of Edinburgh GoogleDeepmind

* **Solving math word problems with process- and outcome-based feedback** [arxiv 2022.11] [pdf](https://arxiv.org/pdf/2211.14275)

  * Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins
  * DeepMind

* **Let's Verify Step by Step** [ICLR 2024] [pdf](https://openreview.net/pdf?id=v8L0pN6EOi) [github](https://github.com/openai/prm800k)

  * Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, Karl Cobbe
  * OpenAI

* **Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations** [ACL 2024] [pdf](https://aclanthology.org/2024.acl-long.510.pdf)

  * Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, Zhifang Sui
  * State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University.  DeepSeek-AI  The University of Hong Kong   Tsinghua University  The Ohio State University

* **Improve Mathematical Reasoning in Language Models by Automated Process Supervision** [arxiv 20224.6] [pdf](https://arxiv.org/pdf/2406.06592)

  * Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, Abhinav Rastogi
  * Google DeepMind, Google

* **Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning** [arxiv 20224.10] [pdf](https://arxiv.org/pdf/2410.08146)

  * Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar
  * Google Research, Google DeepMind, Carnegie Mellon University

* **Critique-out-Loud Reward Models** [arxiv 2024.8] [pdf](https://arxiv.org/pdf/2408.11791) [github](https://github.com/zankner/CLoud)

  * Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang, Prithviraj Ammanabrolu
  * Databricks MIT University of California, San Diego

* **Improving Reward Models with Synthetic Critiques** [arxiv 2024.5] [pdf](https://arxiv.org/pdf/2405.20850)

  * Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, Matthias Gallé
  * University of Oxford   Cohere

* **Generative Verifiers: Reward Modeling as Next-Token Prediction** [arxiv 2024.8] [pdf](https://arxiv.org/pdf/2408.15240)

  * Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal
  * *CoreContribution,1GoogleDeepMind,2UniversityofToronto,3Mila,4UCLA,5CarnegieMellonUniversity

* **Self-Generated Critiques Boost Reward Modeling for Language Models** [arxiv 2024.11] [pdf](https://arxiv.org/pdf/2411.16646)

  * Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou
  * 1GenAI, Meta, 2Georgia Institute of Technology

* **Is ChatGPT a Good NLG Evaluator? A Preliminary Study** [ACL 2023] [pdf](https://aclanthology.org/2023.newsum-1.1.pdf) [github](https://github.com/krystalan/chatgpt_as_nlg_evaluator)

  * Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou
  * Soochow University, Suzhou, China ;Beijing Jiaotong University, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China
    ; Waseda University, Tokyo, Japan ; Fudan Unversity, Shanghai, China

* **ChatGPT as a Factual Inconsistency Evaluator for Text Summarization** [arxiv 2023.3] [pdf](https://arxiv.org/pdf/2303.15621) 

  * Zheheng Luo, Qianqian Xie, Sophia Ananiadou
  * Department of Computer Science, The University of Manchester

* **G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment** [ACL 2023] [pdf ](https://aclanthology.org/2023.emnlp-main.153.pdf) [github](https://github.com/nlpyang/geval)

  * Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu
  * Microsoft Azure AI

* **Can Large Language Models Be an Alternative to Human Evaluations?** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.870.pdf)

  * Cheng-Han Chiang, Hung-yi Lee
  * National Taiwan University

* **LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks** [arxiv 2024.6] [pdf](https://arxiv.org/pdf/2406.18403)

  * Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni
  * University of Amsterdam, 2University of Trento, 3University of Copenhagen,
    4Utrecht University, 5Max Planck Institute for Psycholinguistics, 6ETH Zürich,
    7Saarland University, 8Universidade de Lisboa & Unbabel, 9LMU Munich & MCML,
    10University of Potsdam, 11Heriot-Watt University

* **Large Language Models are not Fair Evaluators** [ACL 2024] [pdf](https://aclanthology.org/2024.acl-long.511.pdf) [github](https://github.com/i-Eval/FairEval)

  * Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifang Sui
  * 1 State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University 2 The University of Hong Kong 3 Tencent Cloud AI

* **Large Language Models are Inconsistent and Biased Evaluators** [arxiv 2024.5] [pdf](https://arxiv.org/pdf/2405.01724)

  * Rickard Stureborg, Dimitris Alikaniotis, Yoshi Suhara
  * 1Grammarly 2Duke University 3NVIDIA

* **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena** [NeurIPS 2023] [pdf](https://openreview.net/pdf?id=uccHPGDlao) [github](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge)

  * Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica
  * 1 UCBerkeley 2 UCSanDiego 3 Carnegie Mellon University 4 Stanford 5 MBZUAI

* **PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization** [ICLR 2024] [pdf](https://openreview.net/pdf?id=5Nn2BLV7SB)

  * Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang
  * Peking University ,Westlake University ,Microsoft Research Asia

 * **JudgeLM: Fine-tuned Large Language Models are Scalable Judges** [arxiv 2023.10] [pdf](https://arxiv.org/pdf/2310.17631) [github](https://github.com/baaivision/JudgeLM)

   * Lianghui Zhu, Xinggang Wang, Xinlong Wang
   * Beijing Academy of Artificial Intelligence;
     School of EIC, Huazhong University of
     Science & Technology

 * **Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging** [arxiv 2024.5] [pdf](https://arxiv.org/pdf/2405.12163) [github](https://github.com/dropreg/Fennec)

   * Xiaobo Liang, Haoke Zhang, Helan hu, Juntao Li, Jun Xu, Min Zhang
   * 1Soochow University, 2Peking University, 3Baidu Inc

 * **REFINER: Reasoning Feedback on Intermediate Representations** [ACL 2024] [pdf](https://aclanthology.org/2024.eacl-long.67.pdf)

   * Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings
   * Université Grenoble Alpes, CNRS, Grenoble INP, LIG

 * **Shepherd: A Critic for Language Model Generation** [arxiv 2023.8] [pdf](https://arxiv.org/pdf/2308.04592) [github](https://arxiv.org/pdf/2308.04592)

   * Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O'Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz
   * Meta AI Research, FAIR

 * **Generative Judge for Evaluating Alignment** [ICLR 2024] [pdf](https://openreview.net/pdf?id=gtkFw6sZGS) [github](https://github.com/GAIR-NLP/auto-j)

   * Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, Pengfei Liu
   * Shanghai Jiao Tong University ;Shanghai Artificial Intelligence Laboratory ;Hong Kong Polytechnic University ;New York University ;Chinese Academy of Sciences ;Generative AI Research Lab (GAIR)

 * **Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate** [arxiv 2024.7] [pdf](https://arxiv.org/pdf/2402.07401)

   * Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling Li, Heng Ji
   * University of Illinois Urbana-Champaign DAMOAcademy, Alibaba Group Northwestern University

   

   ### Search Strategies

 * **Competition-level code generation with alphacode** [arxiv 2022.2] [pdf](https://arxiv.org/pdf/2203.07814) [github](https://github.com/google-deepmind/code_contests)

   * Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals
   * Google-DeepMind

* **Code Llama: Open Foundation Models for Code** [arxiv 2023.8] [pdf](https://arxiv.org/pdf/2308.12950) [github](https://github.com/meta-llama/codellama)

  * Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve
  * Meta AI

* **More Agents Is All You Need** [arxiv 2024.2] [pdf](https://arxiv.org/pdf/2402.05120) [github](https://github.com/MoreAgentsIsAllYouNeed/AgentForest)

  * Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye
  * Tencent

* **Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios** [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.230.pdf)

  * Lei Lin, Jiayi Fu, Pengli Liu, Qingyang Li, Yan Gong, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai
  * Kuaishou Technology, Beijing, China   School of Computer Science and Engineering, Northeastern University, Shenyang, China 

* **Self-Consistency Improves Chain of Thought Reasoning in Language Models** [ICLR 2023] [pdf](https://openreview.net/pdf?id=1PL1NIMMrw)

  * Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou
  * Google Research, Brain Team

* **Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning**[arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.12608) [github](https://github.com/declare-lab/prove)

  * Vernon Y.H. Toh, Deepanway Ghosal, Soujanya Poria
  * Singapore University of Technology and Design

* **Learning to summarize with human feedback**[NeurIPS 2020] [pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf)

  * Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano
  * OpenAI

* **Training Verifiers to Solve Math Word Problems**[arxiv 2021.10] [pdf](https://arxiv.org/pdf/2110.14168)

  * Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman
  * OpenAI

* **WebGPT: Browser-assisted question-answering with human feedback** [arxiv 2021.12] [pdf](https://arxiv.org/pdf/2112.09332)

  * Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman
  * OpenAI

* **Making Language Models Better Reasoners with Step-Aware Verifier** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.291.pdf) [github](https://github.com/microsoft/DiVeRSe)

  * Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen
  * National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University   Microsoft Corporation

* **Accelerating Best-of-N via Speculative Rejection** [ICML 2024] [pdf](https://openreview.net/pdf?id=dRp8tAIPhj)

  * Ruiqi Zhang, Momin Haider, Ming Yin, Jiahao Qiu, Mengdi Wang, Peter Bartlett, Andrea Zanette

* **TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling** [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.16033)

  * Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, Mengdi Wang
  * Department of Electrical & Computer Engineering, Princeton University   Department of Electrical Engineering & Computer Science, University of Michigan  Department of Computer Science, Princeton University  School of Electrical Engineering and Computer Science, Oregon State University  AI Lab, Princeton

* **Fast Best-of-N Decoding via Speculative Rejection** [NeurIPS 2024] [pdf](https://openreview.net/pdf?id=348hfcprUs)

  * Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, Andrea Zanette
  * 1Carnegie Mellon University, 2University of Virginia, 3UC Berkeley 4Princeton University, 5Fudan University, 6Google DeepMind

* **Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation** [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.02725) [github](https://github.com/rohinmanvi/Capability-Aware_and_Mid-Generation_Self-Evaluations)

  * Rohin Manvi, Anikait Singh, Stefano Ermon
  * Stanford University

* **Preference-Guided Reflective Sampling for Aligning Language Models** [EMNLP 2024] [pdf](https://aclanthology.org/2024.emnlp-main.1206.pdf) [github](https://github.com/nusnlp/PRS)

  * Hai Ye, Hwee Tou Ng

  * Department of Computer Science National University of Singapore

* **RARR: Researching and Revising What Language Models Say, Using Language Models** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.910.pdf) [github](https://github.com/anthonywchen/RARR)

  * Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, Kelvin Guu
  * Carnegie Mellon University , Google Research, UC Irvine

* **Reinforced Self-Training (ReST) for Language Modeling**[arxiv 2023.8] [pdf](https://arxiv.org/pdf/2308.08998) 

  * Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando de Freitas
  * 1Google DeepMind, 2Google Research

* **Variational Best-of-N Alignment** [arxiv 2024.7] [pdf](https://arxiv.org/pdf/2407.06057)  

  * Afra Amini, Tim Vieira, Ryan Cotterell

  * ETHZ¨ urich

* **BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling** [NeurIps 2024] [pdf](https://openreview.net/pdf?id=haSKMlrbX5)

  * Lin Gui, Cristina Garbacea, Victor Veitch

  * Department of Statistics, University of Chicago

* **BOND: Aligning LLMs with Best-of-N Distillation** [arxiv 2024.7] [pdf](https://arxiv.org/pdf/2407.14622)

  * Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos, Amélie Héliou, Aliaksei Severyn, Matt Hoffman, Nikola Momchev, Olivier Bachem
  * Google DeepMind

* **Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models** [arxiv 2024.12] [pdf](https://arxiv.org/pdf/2412.15287)

  * Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan, Craig Boutilier, Rishabh Agarwal, Aviral Kumar, Aleksandra Faust
  * 1Google DeepMind, 2Google Research

* **Reflexion: Language Agents with Verbal Reinforcement Learning** [arxiv 2023.3] [pdf](https://arxiv.org/pdf/2303.11366) [github](https://github.com/noahshinn/reflexion)

  * Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao
  * Northeastern University， Massachusetts Institute of Technology，Princeton University

* **Interscript: A dataset for interactive learning of scripts through error feedback** [arxiv 2021.12] [pdf](https://arxiv.org/pdf/2112.07867) [github](https://github.com/allenai/interscript)

  * Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, Yiming Yang
  * Allen Institute for Artificial Intelligence, Seattle, WA, USA   Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA

* **NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction** [ACL 2021] [pdf](https://aclanthology.org/2021.naacl-main.444.pdf) [github](https://github.com/MSR-LIT/NLEdit/)

  * Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, Ahmed Hassan Awadallah
  * University of Maryland   Microsoft Research

* **Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback** [ACL 2022] [pdf](https://aclanthology.org/2022.findings-naacl.26.pdf) [github](https://github.com/allenai/interscript)

  * Niket Tandon, Aman Madaan, Peter Clark, Yiming Yang
  * Allen Institute for Artificial Intelligence, Seattle, WA, USA  Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA

* **CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing** [ICLR 2024] [pdf](https://openreview.net/pdf?id=Sx038qxjek) [github](https://github.com/microsoft/ProphetNet/tree/master/CRITIC)

  * Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, Weizhu Chen
  * Tsinghua University , Microsoft Research Asia, Microsoft Azure AI

* **Teaching Large Language Models to Self-Debug** [ICLR 2024] [pdf](https://openreview.net/pdf?id=KuPixIqPiq)

  * Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou
  * Google DeepMind , UC Berkeley

* **RARR: Researching and Revising What Language Models Say, Using Language Models** [ACL 2023] [pdf](https://aclanthology.org/2023.acl-long.910.pdf) [github](https://github.com/anthonywchen/RARR)

  * Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, Kelvin Guu
  * Carnegie Mellon University , Google Research, UC Irvine

* **Graph-based, Self-Supervised Program Repair from Diagnostic Feedback** [arxiv 2005.10] [pdf](https://arxiv.org/pdf/2005.10636)

  * Michihiro Yasunaga, Percy Liang

* **Improving Factuality and Reasoning in Language Models through Multiagent Debate** [arxiv 2023.5] [pdf](https://arxiv.org/pdf/2305.14325) [Project website](https://arxiv.org/pdf/2305.14325)

  * Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch
  * MIT CSAIL  MIT CSAIL, BCS, CBMM   Google Brain

* **Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate** [EMNLP 2023] [pdf](https://aclanthology.org/2023.findings-emnlp.508.pdf) [github](https://github.com/Waste-Wood/FORD)

  * Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, Bing Qin
  * Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, China ; Singapore Management University, Singapore

* **Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate** [arxiv 2023.5] [pdf](https://arxiv.org/pdf/2305.19118) [github](https://github.com/Skytliang/Multi-Agents-Debate)

  * Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu
  * Tsinghua University, Shanghai JiaoTong University, Tencent AI Lab

* **ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs** [ACL 2024] [pdf](https://aclanthology.org/2024.acl-long.381.pdf) [github](https://github.com/dinobby/ReConcile)

  * Justin Chen, Swarnadeep Saha, Mohit Bansal
  * UNC Chapel Hill

* **Mixture-of-Agents Enhances Large Language Model Capabilities** [arxiv 2024.6] [pdf](https://arxiv.org/pdf/2406.04692) [github](https://github.com/togethercomputer/moa)

  * Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou
  * Duke University  University of Chicago  Stanford University  Together AI

* **Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate** [arxiv 2024.7] [pdf](https://arxiv.org/pdf/2402.07401)

  * Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling Li, Heng Ji
  * University of Illinois Urbana-Champaign DAMOAcademy, Alibaba Group Northwestern University

* **Debating with More Persuasive LLMs Leads to More Truthful Answers** [ICML 2024] [pdf](https://openreview.net/pdf?id=iLCZtl7FTa) [github](https://github.com/ucl-dark/llm_debate)

  * Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, Ethan Perez
  * University College London Speechmatics MATS Anthropic FAR AI.

* **Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate** [EMNLP 2023] [pdf](https://aclanthology.org/2023.findings-emnlp.508.pdf) [github](https://github.com/Waste-Wood/FORD)

  * Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, Bing Qin
  * Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, China ; Singapore Management University, Singapore

* **ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate** [ICLR 2024] [pdf ](https://openreview.net/pdf?id=FQepisCUWu)

  * Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu
  * Tsinghua University  Hong Kong University of Science and Technology  Peking University

* **ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting** [IREC 2024] [pdf](https://aclanthology.org/2024.lrec-main.265.pdf)

  * Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen
  * Gaoling School of Artificial Intelligence, Renmin University of China 2School of Information,Renmin University of China  3DIRO, Université de Montréal

* **Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment** [arxiv 2023.11] [pdf](https://arxiv.org/pdf/2311.08596)

  * Philippe Laban, Lidiya Murakhovs'ka, Caiming Xiong, Chien-Sheng Wu
  * Salesforce AI Research

* **MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate** [arxiv 2024.6] [pdf](https://arxiv.org/pdf/2406.14711) 

  * Alfonso Amayuelas, Xianjun Yang, Antonis Antoniades, Wenyue Hua, Liangming Pan, William Wang
  * UC Santa Barbara, Rutgers University

* **Teaching Models to Balance Resisting and Accepting Persuasion** [arxiv 2024.10] [pdf][https://arxiv.org/pdf/2410.14596] [github](https://github.com/esteng/persuasion_balanced_training)

  * Elias Stengel-Eskin, Peter Hase, Mohit Bansal
  * UNCChapel Hill Abstract Anthropic

* **GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion** [arxiv 2024.9] [pdf](https://arxiv.org/pdf/2409.14051)

  * Tongxuan Liu, Xingyu Wang, Weizhe Huang, Wenjiang Xu, Yuting Zeng, Lei Jiang, Hailong Yang, Jing Li
  * University of Science and Technology of China 2Institute of Automation, Chinese Academy of Sciences  Beihang University

* **Improving Multi-Agent Debate with Sparse Communication Topology** [arxiv 2024.6] [pdf](https://arxiv.org/pdf/2406.11776)

  * Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, Eugene Ie
  * Google , Google DeepMind

* **Self-Rewarding Language Models** [arxiv 2024.1] [pdf](https://arxiv.org/pdf/2401.10020)

  * Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, Jason Weston
  * Meta NYU

* **Constitutional AI: Harmlessness from AI Feedback** [arxiv 2022.12] [pdf](https://arxiv.org/pdf/2212.08073) [github](https://github.com/anthropics/ConstitutionalHarmlessnessPaper)

  * Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Jared Kaplan
  * Anthropic

* **Self-Refine: Iterative Refinement with Self-Feedback** [NeurIPS 2023] [pdf](https://openreview.net/pdf?id=S37hOerQLB)

  * Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark
  * Language Technologies Institute, Carnegie Mellon University; Allen Institute for Artificial Intelligence ;University of Washington; NVIDIA ;UC San Diego ;Google Deepmind

* **Language Models can Solve Computer Tasks** [arxiv 2023.3] [pdf](https://arxiv.org/pdf/2303.17491) [github](https://github.com/posgnu/rci-agent)

  * Geunwoo Kim, Pierre Baldi, Stephen McAleer
  * University of California, Irvine ; Carnegie Mellon University

* **Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models** [arxiv 2024.2] [pdf](https://arxiv.org/pdf/2402.12563) [github](https://github.com/MBZUAI-CLeaR/IoE-Prompting)

  * Loka Li, Zhenhao Chen, Guangyi Chen, Yixuan Zhang, Yusheng Su, Eric Xing, Kun Zhang
  * Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE Carnegie Mellon University, Pittsburgh PA, USA

* **Is Self-Repair a Silver Bullet for Code Generation?** [ICLR 2024] [pdf](https://openreview.net/pdf?id=y0GJXRungR) [github](https://github.com/theoxo/self-repair)

  * Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, Armando Solar-Lezama
  * MIT CSAIL Microsoft Research

* **Large Language Models Cannot Self-Correct Reasoning Yet** [ICLR 2024] [pdf](https://openreview.net/pdf?id=IkmD3fKBPQ)

  * Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, Denny Zhou
  * Google DeepMind  University of Illinois at Urbana-Champaign

* **Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies** [arxiv 2024.6] [pdf](https://arxiv.org/pdf/2406.06461)

  * Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, Ben Athiwaratkun
  * Amazon

* **Can Large Language Models Really Improve by Self-critiquing Their Own Plans?** [arxiv 2023.10] [pdf](https://arxiv.org/pdf/2310.08118)

  * Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati
  * School of Computing & AI Arizona State University Tempe.

* **GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems** [arxiv 2023.10] [pdf](https://arxiv.org/pdf/2310.12397)

  * Kaya Stechly, Matthew Marquez, Subbarao Kambhampati
  * Arizona State University, Tempe

* **When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs** [arxiv 2024.6] [pdf](https://arxiv.org/pdf/2406.01297) 

  * Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, Rui Zhang
  * Penn State University , University of Illinois Urbana-Champaign

* **LLMs cannot find reasoning errors, but can correct them given the error location** [ACL 2024] [pdf](https://aclanthology.org/2024.findings-acl.826.pdf) [github](https://github.com/WHGTyen/BIG-Bench-Mistake)

  * Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, Tony Mak
  * University of Cambridge, Dept. of Computer Science & Technology, ALTA Institute ; Google Research

* **Self-critiquing models for assisting human evaluators** [arxiv 2022.6] [pdf](https://arxiv.org/pdf/2206.05802)

  * William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike
  * OpenAI

* **Recursive Introspection: Teaching Language Model Agents How to Self-Improve** [arxiv 2024.7] [pdf](https://arxiv.org/pdf/2407.18219)

  * Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar
  * Carnegie Mellon University, UC Berkeley, MultiOn

* **Embedding Self-Correction as an Inherent Ability in Large Language Models for Enhanced Mathematical Reasoning** [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.10735)

  * Kuofeng Gao, Huanqia Cai, Qingyao Shuai, Dihong Gong, Zhifeng Li
  * Tencent, Shenzhen, Guangdong, China

* **Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning** [arxiv 2024.6] [pdf](https://arxiv.org/pdf/2406.12050) [github](https://github.com/ytyz1307zzh/RefAug)

  * Zhihan Zhang, Tao Ge, Zhenwen Liang, Wenhao Yu, Dian Yu, Mengzhao Jia, Dong Yu, Meng Jiang
  * University of Notre Dame Tencent AI Lab, Seattle

* **GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements** [arxiv 2024.2] [pdf](https://arxiv.org/pdf/2402.10963) 

  * Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Raileanu
  * FAIR at Meta, Georgia Institute of Technology, StabilityAI

* **Generating Sequences by Learning to Self-Correct** [ICLR 2023] [pdf](https://openreview.net/pdf?id=hH36JeQZDaO) [github](https://github.com/wellecks/self_correction)

  * Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi
  * Allen Institute for Artificial Intelligence ;Center for Language and Speech Processing, Johns Hopkins University ;Paul G. Allen School of Computer Science & Engineering, University of Washington

* **Training Language Models to Self-Correct via Reinforcement Learning** [arxiv 2024.9] [pdf](https://arxiv.org/pdf/2409.12917)

  * Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, Aleksandra Faust
  * Google DeepMind

* **Tree of Thoughts: Deliberate Problem Solving with Large Language Models** [arxiv 2023.5] [pdf](https://arxiv.org/pdf/2305.10601) [github](https://github.com/princeton-nlp/tree-of-thought-llm)

  * Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan
  * Princeton University  Google DeepMind  Princeton University

* **Self-Evaluation Guided Beam Search for Reasoning** [NeurIPS 2023] [pdf](https://openreview.net/pdf?id=Bw82hwg5Q3) [github](https://github.com/YuxiXie/SelfEval-Guided-Decoding)

  * Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, Qizhe Xie
  * National University of Singapore   The Hong Kong University of Science and Technology

* **Reasoning with Language Model is Planning with World Model** [EMNLP 2023] [pdf](https://aclanthology.org/2023.emnlp-main.507.pdf) [github](https://github.com/maitrix-org/llm-reasoners)

  * Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, Zhiting Hu
  * UCSan Diego, University of Florida  Mohamed bin Zayed University of Artificial Intelligence

* **Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B** [arxiv 2024.6] [pdf](https://arxiv.org/pdf/2406.07394) [github](https://github.com/trotsky1997/MathBlackBox)

  * Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang
  * Fudan University , Shanghai Artificial Intelligence Laboratory

* **Reasoning with Language Model is Planning with World Model** [EMNLP 2023] [pdf](https://aclanthology.org/2023.emnlp-main.507.pdf) [github](https://github.com/maitrix-org/llm-reasoners)

  * Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, Zhiting Hu
  * UCSan Diego, University of Florida  Mohamed bin Zayed University of Artificial Intelligence

* **ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs** [ACL 2024] [pdf](https://aclanthology.org/2024.acl-long.381.pdf) [github](https://github.com/dinobby/ReConcile)

  * Justin Chen, Swarnadeep Saha, Mohit Bansal
  * UNC Chapel Hill

* **Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training** [arxiv 2023.9] [pdf](https://arxiv.org/pdf/2309.17179)

  * Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, Jun Wang
  * University College London ,Shanghai Jiao Tong University ,Carnegie Mellon University

* **Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers** [arxiv 2024.8] [pdf](https://arxiv.org/pdf/2408.06195) [github](https://github.com/zhentingqi/rStar)
  * Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang
  * Microsoft Research Asia , Harvard University

* **Interpretable Contrastive Monte Carlo Tree Search Reasoning** [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.01707) [github](https://github.com/zitian-gao/SC-MCTS)
  * Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, Lijie Wen
  * 1The University of Sydney 2Peking University 3Xiaohongshu Inc. 4Shanghai AI Laboratory
    5Tsinghua University 6The Hong Kong University of Science and Technology
    7The Hong Kong University of Science and Technology (Guangzhou)
* **ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search** [arxiv 2024.6] [pdf ](https://arxiv.org/pdf/2406.03816) [github](https://github.com/THUDM/ReST-MCTS)
  * Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang
  * The Knowledge Engineering Group (KEG), Tsinghua University; California Institute of Technology
* **Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning** [arxiv 2024.5] [pdf](https://arxiv.org/pdf/2405.00451) [github](https://github.com/YuxiXie/MCTS-DPO)
  * Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, Michael Shieh
  * National University of Singapore; Google DeepMind

* **O1 Replication Journey: A Strategic Progress Report -- Part 1** [arxiv 2024.10] [pdf](https://arxiv.org/pdf/2410.18982) [github](https://github.com/GAIR-NLP/O1-Journey)
  * Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, Pengfei Liu
  * Shanghai Jiao Tong University, New York University,
    MBZUAI, Generative AI Research Lab (GAIR)

* **Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions** [arxiv 2024.11] [pdf](https://arxiv.org/pdf/2411.14405) [github](https://github.com/AIDC-AI/Marco-o1)
  * Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang
  * MarcoPolo Team, Alibaba International Digital Commerce

* **o1-Coder: an o1 Replication for Coding** [arxiv 2024.12] [pdf](https://arxiv.org/pdf/2412.00154) [github](https://github.com/ADaM-BJTU/o1-coder)
  * Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, Jitao Sang
  * School of Computer Science and Technology , Beijing Jiaotong University









##  Future Directions
### Generalizable System-2 Model
* **DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging** [arxiv 2024.10.5] [pdf](https://arxiv.org/pdf/2407.01470) [github](https://github.com/MiuLab/DogeRM)
  * Tzu-Han Lin, Chen-An Li, Hung-yi Lee, Yun-Nung Chen
  * National Taiwan University, Taipei, Taiwan
* **Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs** [arxiv 2024.10.23] [pdf](https://arxiv.org/pdf/2406.10216) [github](https://github.com/YangRui2015/Generalizable-Reward-Model)
  * Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, Tong Zhang
  * University of Illinois Urbana-Champaign, Georgia Institute of Technology, Princeton University, Princeton Language and Intelligence
* **Generalizing Reward Modeling for Out-of-Distribution Preference Learning** [arxiv 2024.6.8] [pdf](https://arxiv.org/pdf/2402.14760) [github](https://github.com/jiachenwestlake/OODPL)
  * Chen Jia
  * SI-TECH Information Technology
* **Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision** [arxiv 2023.12.24] [pdf](https://arxiv.org/pdf/2312.09390)
  * Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, Jeff Wu
  * OpenAI
### Multimodal Reasoning
* **Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models** [arxiv 2023.5.29] [pdf](https://arxiv.org/pdf/2305.18010) [github](https://github.com/lil-lab/bandit-qa)
  * Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yi Yang
  * ReLER Lab, AAII, University of Technology Sydney, ReLER Lab, CCAI, Zhejiang University, Stanford University, Baidu Inc.
* **Multimodal Chain-of-Thought Reasoning in Language Models** [arxiv 2024.5.20] [pdf](https://arxiv.org/pdf/2302.00923) [github](https://github.com/amazon-science/mm-cot)
  * Zhuosheng Zhang, Aston Zhang, Mu Li, hai zhao, George Karypis, Alex Smola
  * School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University. GenAI, Meta. Amazon Web Services
* **Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models** [arxiv 2024.10.24] [pdf](https://arxiv.org/pdf/2404.03622)
  * Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei
  * Microsoft Research, East China Normal University
* **KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning** [arxiv 2024.1.23] [pdf](https://arxiv.org/pdf/2401.12863)
  * Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, Godawari Sudhakar Rao
  * Samsung R&D Institute India - Bangalore
* **Multimodal Reasoning with Multimodal Knowledge Graph** [arxiv 2024.6.5] [pdf](https://arxiv.org/pdf/2406.02030)
  * Junlin Lee, Yequan Wang, Jing Li, Min Zhang
  * Harbin Institute of Technology, Shenzhen, China. Beijing Academy of Artificial Intelligence, Beijing, China
* **Interleaved-Modal Chain-of-Thought** [arxiv 2024.11.29] [pdf](https://arxiv.org/pdf/2411.19488)
  * Jun Gao, Yongqi Li, Ziqiang Cao, Wenjie Li
  * School of Computer Science and Technology, Soochow University. Department of Computer Science, The Hong Kong Polytechnic University
* **LLaVA-CoT: Let Vision Language Models Reason Step-by-Step** [arxiv 2024.11.25] [pdf](https://arxiv.org/pdf/2411.10440) [github](https://github.com/PKU-YuanGroup/LLaVA-CoT)
  * Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan
  * School of Electronic and Computer Engineering, Peking University. Institute for Interdisciplinary Information Sciences, Tsinghua University. Rabbitpre AI & PKU Shenzhen AIGC Joint Lab. Peng Cheng Laboratory. 5DAMO Academy, Alibaba Group. 6Hupan Lab. Computer Science and Engineering, Lehigh University
### Efficiency and Performance Trade-off
* **Learning How Hard to Think: Input-Adaptive Allocation of LM Computation** [arxiv 2024.10.7] [pdf](https://arxiv.org/pdf/2410.04707)
  * Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, Jacob Andreas
  * Massachusetts Institute of Technology
* **Scaling LLM Inference with Optimized Sample Compute Allocation** [arxiv 2024.10.29] [pdf](https://arxiv.org/pdf/2410.22480) [github](https://github.com/LeiLiLab/OSCA)
  * Kexun Zhang, Shang Zhou, Danqing Wang, William Yang Wang, Lei Li
  * 1Carnegie Mellon University. UC San Diego. UC Santa Barbara
* **Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies** [arxiv 2024.6.15] [pdf](https://arxiv.org/pdf/2406.06461)
  * Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, Ben Athiwaratkun
  * Amazon
* **Token-Budget-Aware LLM Reasoning** [arxiv 2024.12.30] [pdf](https://arxiv.org/pdf/2412.18547) [github](https://github.com/GeniusHTX/TALE)
  * Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, Zhenting Wang
  * Nanjing University. 2Rutgers University. UMass Amherst
* **Compressed Chain of Thought: Efficient Reasoning Through Dense Representations** [arxiv 2024.12.17] [pdf](https://arxiv.org/pdf/2412.13171)
  * Jeffrey Cheng, Benjamin Van Durme
  * Department of Computer Science, Johns Hopkins University, Baltimore, US.
### Unified Scaling Law
* **Large Language Monkeys: Scaling Inference Compute with Repeated Sampling** [arxiv 2024.9.16] [pdf](https://arxiv.org/pdf/2407.21787)
  * Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, Azalia Mirhoseini
  * Department of Computer Science, Stanford University. University of Oxford. Google DeepMind
* **Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters** [arxiv 2024.8.6] [pdf](https://arxiv.org/pdf/2408.03314)
  * Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  * Equal advising, UC Berkeley, Google DeepMind
